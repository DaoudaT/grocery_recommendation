{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grocery Recommendation Project\n",
    "\n",
    "1. Data Clustering on users  \n",
    "2. Recommendation systems\n",
    "  * Content-based filtering  \n",
    "  * Collaborative filtering  \n",
    "3. Metadata NLP search engine\n",
    "4. Market Basket analysis  \n",
    "4. Website interface  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis I will be working as a Data Scientist for a grocery store that is looking to discover insights from sales data that could be used for targeted direct mail marketing (specific coupons mailed to customers), targeted email marketing (\"An item you like has gone on sale!\"), and online shopper recommendations to 'add to cart' based on similar items and also based on items other people who bought that item have purchased.  \n",
    "\n",
    "If time permits, I may also perform a market basket analysis to forecast what products a customer is likely to purchase in their next order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This data was retrieved from Kaggle and was provided by Instacart for a market basket analysis competition in 2018.  \n",
    "\n",
    "The data is divided into 6 files:\n",
    "\n",
    "- **_Aisles.csv_**: 134 Unique aisle numbers and descriptions\n",
    "- **_Departments.csv_**: 21 Unique department numbers and descriptions\n",
    "- **_Products.csv_**: 49,688 Unique product ids, with description, aisle id, and department id\n",
    "- **_Orders.csv_**: 3,421,083 Unique order id, with user id, order number, order_dow, order_hour_of_day, days_since_prior_order, and eval_set indicating if the order is in train, prior, or test\n",
    "- **_Order_products_train.csv_**: Order id, product id, add to cart order, and reorder indicator\n",
    "- **_Order_products_prior.csv_**: Order id, product id, add to cart order, and reorder indicator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## EDA and Data Preprocessing\n",
    "\n",
    "See separate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import time\n",
    "from user_functions import *\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set default visualization parameters\n",
    "\n",
    "CB91_Blue = '#2CBDFE'\n",
    "CB91_Green = '#47DBCD'\n",
    "CB91_Pink = '#F3A0F2'\n",
    "CB91_Purple = '#9D2EC5'\n",
    "CB91_Violet = '#661D98'\n",
    "CB91_Amber = '#F5B14C'\n",
    "color_list = [CB91_Blue, CB91_Pink, CB91_Green, CB91_Amber, CB91_Purple, CB91_Violet]\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=color_list)\n",
    "sns.set_context(\"notebook\", rc={\"font.size\":16, \"axes.titlesize\":20, \"axes.labelsize\":18})\n",
    "sns.set(font='Franklin Gothic Book',\n",
    "rc={'axes.axisbelow': False,\n",
    "'axes.edgecolor': 'lightgrey',\n",
    "# 'axes.edgecolor': 'white',\n",
    "'axes.facecolor': 'None',\n",
    "'axes.grid': False,\n",
    "'axes.labelcolor': 'dimgrey',\n",
    "# 'axes.labelcolor': 'white',\n",
    "'axes.spines.right': False,\n",
    "'axes.spines.top': False,\n",
    "'axes.prop_cycle': plt.cycler(color=color_list),\n",
    "'figure.facecolor': 'white',\n",
    "'lines.solid_capstyle': 'round',\n",
    "'patch.edgecolor': 'w',\n",
    "'patch.force_edgecolor': True,\n",
    "'text.color': 'dimgrey',\n",
    "# 'text.color': 'white',    \n",
    "'xtick.bottom': False,\n",
    "'xtick.color': 'dimgrey',\n",
    "# 'xtick.color': 'white',    \n",
    "'xtick.direction': 'out',\n",
    "'xtick.top': False,\n",
    "'ytick.color': 'dimgrey',\n",
    "# 'ytick.color': 'white',\n",
    "'ytick.direction': 'out',\n",
    "'ytick.left': False,\n",
    "'ytick.right': False})\n",
    "%matplotlib inline\n",
    "\n",
    "# NOTE: if you visualizations are too cluttered to read, try calling 'plt.gcf().autofmt_xdate()'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now I want to experiment with clustering the 'similar' users together\n",
    "# But what data do I need for each user?  Some kind of summary statistics?\n",
    "# I guess I need each product to be a column, with the number of times it was ordered?\n",
    "# Product level is too granular so I am going to try aisle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_orders = pickle.load(open(\"Pickle/merged_orders.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "      <th>aisle</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539329</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Soda</td>\n",
       "      <td>77</td>\n",
       "      <td>7</td>\n",
       "      <td>soft drinks</td>\n",
       "      <td>beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2539329</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26088</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Aged White Cheddar Popcorn</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>popcorn jerky</td>\n",
       "      <td>snacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2539329</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14084</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Organic Unsweetened Vanilla Almond Milk</td>\n",
       "      <td>91</td>\n",
       "      <td>16</td>\n",
       "      <td>soy lactosefree</td>\n",
       "      <td>dairy eggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2539329</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26405</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>XL Pick-A-Size Paper Towel Rolls</td>\n",
       "      <td>54</td>\n",
       "      <td>17</td>\n",
       "      <td>paper goods</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2539329</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12427</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Original Beef Jerky</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>popcorn jerky</td>\n",
       "      <td>snacks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
       "0   2539329        1    prior             1          2                  8   \n",
       "1   2539329        1    prior             1          2                  8   \n",
       "2   2539329        1    prior             1          2                  8   \n",
       "3   2539329        1    prior             1          2                  8   \n",
       "4   2539329        1    prior             1          2                  8   \n",
       "\n",
       "   days_since_prior_order  product_id  add_to_cart_order  reordered  \\\n",
       "0                     NaN         196                  1          0   \n",
       "1                     NaN       26088                  4          0   \n",
       "2                     NaN       14084                  2          0   \n",
       "3                     NaN       26405                  5          0   \n",
       "4                     NaN       12427                  3          0   \n",
       "\n",
       "                              product_name  aisle_id  department_id  \\\n",
       "0                                     Soda        77              7   \n",
       "1               Aged White Cheddar Popcorn        23             19   \n",
       "2  Organic Unsweetened Vanilla Almond Milk        91             16   \n",
       "3         XL Pick-A-Size Paper Towel Rolls        54             17   \n",
       "4                      Original Beef Jerky        23             19   \n",
       "\n",
       "             aisle  department  \n",
       "0      soft drinks   beverages  \n",
       "1    popcorn jerky      snacks  \n",
       "2  soy lactosefree  dairy eggs  \n",
       "3      paper goods   household  \n",
       "4    popcorn jerky      snacks  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What don't I need for my user dataframe?\n",
    "# Since product level is too granular, I will capture the aisle\n",
    "user_info = merged_orders[['user_id', 'order_number', 'order_dow', 'order_hour_of_day', \n",
    "                           'days_since_prior_order', 'aisle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>aisle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>soft drinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>popcorn jerky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>soy lactosefree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>paper goods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>popcorn jerky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  order_number  order_dow  order_hour_of_day  \\\n",
       "0        1             1          2                  8   \n",
       "1        1             1          2                  8   \n",
       "2        1             1          2                  8   \n",
       "3        1             1          2                  8   \n",
       "4        1             1          2                  8   \n",
       "\n",
       "   days_since_prior_order            aisle  \n",
       "0                     NaN      soft drinks  \n",
       "1                     NaN    popcorn jerky  \n",
       "2                     NaN  soy lactosefree  \n",
       "3                     NaN      paper goods  \n",
       "4                     NaN    popcorn jerky  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dummy variables for each aisle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.get_dummies(user_info, prefix=None, columns=['aisle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>aisle_air fresheners candles</th>\n",
       "      <th>aisle_asian foods</th>\n",
       "      <th>aisle_baby accessories</th>\n",
       "      <th>aisle_baby bath body care</th>\n",
       "      <th>aisle_baby food formula</th>\n",
       "      <th>...</th>\n",
       "      <th>aisle_spreads</th>\n",
       "      <th>aisle_tea</th>\n",
       "      <th>aisle_tofu meat alternatives</th>\n",
       "      <th>aisle_tortillas flat bread</th>\n",
       "      <th>aisle_trail mix snack mix</th>\n",
       "      <th>aisle_trash bags liners</th>\n",
       "      <th>aisle_vitamins supplements</th>\n",
       "      <th>aisle_water seltzer sparkling water</th>\n",
       "      <th>aisle_white wines</th>\n",
       "      <th>aisle_yogurt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33819101</th>\n",
       "      <td>206209</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33819102</th>\n",
       "      <td>206209</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33819103</th>\n",
       "      <td>206209</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33819104</th>\n",
       "      <td>206209</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33819105</th>\n",
       "      <td>206209</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33819106 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  order_number  order_dow  order_hour_of_day  \\\n",
       "0               1             1          2                  8   \n",
       "1               1             1          2                  8   \n",
       "2               1             1          2                  8   \n",
       "3               1             1          2                  8   \n",
       "4               1             1          2                  8   \n",
       "...           ...           ...        ...                ...   \n",
       "33819101   206209            14          6                 14   \n",
       "33819102   206209            14          6                 14   \n",
       "33819103   206209            14          6                 14   \n",
       "33819104   206209            14          6                 14   \n",
       "33819105   206209            14          6                 14   \n",
       "\n",
       "          days_since_prior_order  aisle_air fresheners candles  \\\n",
       "0                            NaN                             0   \n",
       "1                            NaN                             0   \n",
       "2                            NaN                             0   \n",
       "3                            NaN                             0   \n",
       "4                            NaN                             0   \n",
       "...                          ...                           ...   \n",
       "33819101                    30.0                             0   \n",
       "33819102                    30.0                             0   \n",
       "33819103                    30.0                             0   \n",
       "33819104                    30.0                             0   \n",
       "33819105                    30.0                             0   \n",
       "\n",
       "          aisle_asian foods  aisle_baby accessories  \\\n",
       "0                         0                       0   \n",
       "1                         0                       0   \n",
       "2                         0                       0   \n",
       "3                         0                       0   \n",
       "4                         0                       0   \n",
       "...                     ...                     ...   \n",
       "33819101                  0                       0   \n",
       "33819102                  0                       0   \n",
       "33819103                  0                       0   \n",
       "33819104                  0                       0   \n",
       "33819105                  0                       0   \n",
       "\n",
       "          aisle_baby bath body care  aisle_baby food formula  ...  \\\n",
       "0                                 0                        0  ...   \n",
       "1                                 0                        0  ...   \n",
       "2                                 0                        0  ...   \n",
       "3                                 0                        0  ...   \n",
       "4                                 0                        0  ...   \n",
       "...                             ...                      ...  ...   \n",
       "33819101                          0                        0  ...   \n",
       "33819102                          0                        0  ...   \n",
       "33819103                          0                        0  ...   \n",
       "33819104                          0                        0  ...   \n",
       "33819105                          0                        0  ...   \n",
       "\n",
       "          aisle_spreads  aisle_tea  aisle_tofu meat alternatives  \\\n",
       "0                     0          0                             0   \n",
       "1                     0          0                             0   \n",
       "2                     0          0                             0   \n",
       "3                     0          0                             0   \n",
       "4                     0          0                             0   \n",
       "...                 ...        ...                           ...   \n",
       "33819101              0          0                             0   \n",
       "33819102              0          0                             0   \n",
       "33819103              0          0                             0   \n",
       "33819104              0          0                             0   \n",
       "33819105              0          0                             0   \n",
       "\n",
       "          aisle_tortillas flat bread  aisle_trail mix snack mix  \\\n",
       "0                                  0                          0   \n",
       "1                                  0                          0   \n",
       "2                                  0                          0   \n",
       "3                                  0                          0   \n",
       "4                                  0                          0   \n",
       "...                              ...                        ...   \n",
       "33819101                           0                          0   \n",
       "33819102                           0                          0   \n",
       "33819103                           0                          0   \n",
       "33819104                           0                          0   \n",
       "33819105                           0                          0   \n",
       "\n",
       "          aisle_trash bags liners  aisle_vitamins supplements  \\\n",
       "0                               0                           0   \n",
       "1                               0                           0   \n",
       "2                               0                           0   \n",
       "3                               0                           0   \n",
       "4                               0                           0   \n",
       "...                           ...                         ...   \n",
       "33819101                        0                           0   \n",
       "33819102                        0                           0   \n",
       "33819103                        0                           0   \n",
       "33819104                        0                           0   \n",
       "33819105                        0                           0   \n",
       "\n",
       "          aisle_water seltzer sparkling water  aisle_white wines  aisle_yogurt  \n",
       "0                                           0                  0             0  \n",
       "1                                           0                  0             0  \n",
       "2                                           0                  0             0  \n",
       "3                                           0                  0             0  \n",
       "4                                           0                  0             0  \n",
       "...                                       ...                ...           ...  \n",
       "33819101                                    0                  0             0  \n",
       "33819102                                    0                  0             0  \n",
       "33819103                                    0                  0             0  \n",
       "33819104                                    0                  0             0  \n",
       "33819105                                    0                  0             0  \n",
       "\n",
       "[33819106 rows x 139 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data\n",
    "# Group By User_id\n",
    "# I need max of order_number\n",
    "# Mode of order_dow, median of order_hour_of_day, mean of days_since\n",
    "# Sum of each aisle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    33445290\n",
       "1      373816\n",
       "Name: aisle_soft drinks, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data['aisle_soft drinks'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By each User Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data in thirds to use groupby then will rejoin them\n",
    "user_data1 = user_data[user_data['user_id'] <= 65000]\n",
    "user_data2 = user_data[(user_data['user_id'] <= 135000) & (user_data['user_id'] > 65000)]\n",
    "user_data3 = user_data[user_data['user_id'] > 135000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_user1 = user_data1.groupby('user_id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 11.5 GiB for an array with shape (134, 11508594) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-491c43addaa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrouped_user2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_data2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1537\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_groupby_agg_method_template\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1539\u001b[1;33m         return self._agg_general(\n\u001b[0m\u001b[0;32m   1540\u001b[0m             \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"add\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1541\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_agg_general\u001b[1;34m(self, numeric_only, min_count, alias, npfunc)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[1;31m# try a cython aggregation if we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m             return self._cython_agg_general(\n\u001b[0m\u001b[0;32m   1000\u001b[0m                 \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnpfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m     ) -> DataFrame:\n\u001b[1;32m-> 1021\u001b[1;33m         agg_blocks, agg_items = self._cython_agg_blocks(\n\u001b[0m\u001b[0;32m   1022\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             \u001b[0mlocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m                 result, _ = self.grouper.aggregate(\n\u001b[0m\u001b[0;32m   1051\u001b[0m                     \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\groupby\\ops.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, values, how, axis, min_count)\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     ) -> Tuple[np.ndarray, Optional[List[str]]]:\n\u001b[1;32m--> 584\u001b[1;33m         return self._cython_operation(\n\u001b[0m\u001b[0;32m    585\u001b[0m             \u001b[1;34m\"aggregate\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\groupby\\ops.py\u001b[0m in \u001b[0;36m_cython_operation\u001b[1;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[0;32m    494\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_float64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_int_or_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_complex_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_float64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mensure_int_or_float\u001b[1;34m(arr, copy)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;31m# TODO: GH27506 potential bug with ExtensionArrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"int64\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"safe\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.5 GiB for an array with shape (134, 11508594) and data type int64"
     ]
    }
   ],
   "source": [
    "grouped_user2 = user_data2.groupby('user_id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_user3 = user_data3.groupby('user_id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back to user_data and drop the aisle info before I group the other variables in different ways\n",
    "user_data1_noaisles = user_data1.iloc[:,:5]\n",
    "user_data2_noaisles = user_data2.iloc[:,:5]\n",
    "user_data3_noaisles = user_data3.iloc[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cool.  Found a way to group each varaiable differently.\n",
    "group1 = user_data1_noaisles.groupby('user_id').agg({'order_number': 'max', 'order_dow': lambda x:x.value_counts().index[0], \n",
    "                                            'order_hour_of_day': 'median', 'days_since_prior_order': 'mean'})\n",
    "group2 = user_data2_noaisles.groupby('user_id').agg({'order_number': 'max', 'order_dow': lambda x:x.value_counts().index[0], \n",
    "                                            'order_hour_of_day': 'median', 'days_since_prior_order': 'mean'})\n",
    "group3 = user_data3_noaisles.groupby('user_id').agg({'order_number': 'max', 'order_dow': lambda x:x.value_counts().index[0], \n",
    "                                            'order_hour_of_day': 'median', 'days_since_prior_order': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of orders for each user and add to grouped_user dfs\n",
    "grouped_user1['num_orders'] = group1.order_number\n",
    "grouped_user2['num_orders'] = group2.order_number\n",
    "grouped_user3['num_orders'] = group3.order_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_user1['mean_days_since'] = group1.days_since_prior_order\n",
    "grouped_user2['mean_days_since'] = group2.days_since_prior_order\n",
    "grouped_user3['mean_days_since'] = group3.days_since_prior_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_user1['mode_order_dow'] = group1.order_dow\n",
    "grouped_user2['mode_order_dow'] = group2.order_dow\n",
    "grouped_user3['mode_order_dow'] = group3.order_dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_user1['median_order_hour'] = group1.order_hour_of_day\n",
    "grouped_user2['median_order_hour'] = group2.order_hour_of_day\n",
    "grouped_user3['median_order_hour'] = group3.order_hour_of_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_user1.drop(columns=['order_number', 'order_dow', 'order_hour_of_day', 'days_since_prior_order'], inplace=True)\n",
    "grouped_user2.drop(columns=['order_number', 'order_dow', 'order_hour_of_day', 'days_since_prior_order'], inplace=True)\n",
    "grouped_user3.drop(columns=['order_number', 'order_dow', 'order_hour_of_day', 'days_since_prior_order'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_users = pd.concat([grouped_user1, grouped_user2, grouped_user3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function came from a Medium article by Adam Ross Nelson to rearrange columns in a df\n",
    "def movecol(df, cols_to_move=[], ref_col='', place='After'):\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    if place == 'After':\n",
    "        seg1 = cols[:list(cols).index(ref_col) + 1]\n",
    "        seg2 = cols_to_move\n",
    "    if place == 'Before':\n",
    "        seg1 = cols[:list(cols).index(ref_col)]\n",
    "        seg2 = cols_to_move + [ref_col]\n",
    "    \n",
    "    seg1 = [i for i in seg1 if i not in seg2]\n",
    "    seg3 = [i for i in cols if i not in seg1 + seg2]\n",
    "    \n",
    "    return(df[seg1 + seg2 + seg3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped_users = movecol(grouped_users, \n",
    "             cols_to_move=['num_orders', 'mode_order_dow', 'median_order_hour', 'mean_days_since'], \n",
    "             ref_col='aisle_air fresheners candles',\n",
    "             place='Before')\n",
    "grouped_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(grouped_users, open(\"Pickle/grouped_users.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_users = scaler.fit_transform(grouped_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "random_state = 12\n",
    "\n",
    "'''The classical EM-style algorithm is “full”. The “elkan” variation is more efficient on data with well-defined clusters,\n",
    "by using the triangle inequality. However it’s more memory intensive due to the allocation of an extra array of shape \n",
    "(n_samples, n_clusters).'''\n",
    "\n",
    "# Is having 20 separate market segments helpful?  Can I figure out what make them different and target that?  \n",
    "\n",
    "k_means_13 = KMeans(n_clusters=13, random_state=random_state, algorithm='full').fit(scaled_users)\n",
    "k_means_14 = KMeans(n_clusters=14, random_state=random_state, algorithm='full').fit(scaled_users)\n",
    "k_means_15 = KMeans(n_clusters=15, random_state=random_state, algorithm='full').fit(scaled_users)\n",
    "k_means_16 = KMeans(n_clusters=16, random_state=random_state, algorithm='full').fit(scaled_users)\n",
    "k_means_17 = KMeans(n_clusters=17, random_state=random_state, algorithm='full').fit(scaled_users)\n",
    "k_means_18 = KMeans(n_clusters=18, random_state=random_state, algorithm='full').fit(scaled_users)\n",
    "k_means_19 = KMeans(n_clusters=19, random_state=random_state, algorithm='full').fit(scaled_users)\n",
    "k_means_10 = KMeans(n_clusters=10, random_state=random_state, algorithm='full').fit(scaled_users)\n",
    "k_means_11 = KMeans(n_clusters=11, random_state=random_state, algorithm='full').fit(scaled_users)\n",
    "k_means_12 = KMeans(n_clusters=12, random_state=random_state, algorithm='full').fit(scaled_users)\n",
    "# k_means_20 = KMeans(n_clusters=20, random_state=random_state, algorithm='full').fit(scaled_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(k_means_20, open(\"Pickle/k_means_20.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_20 = pickle.load(open(\"Pickle/k_means_20.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [k_means_10, k_means_11, k_means_12, k_means_13, \n",
    "          k_means_14, k_means_15, k_means_16, k_means_17, k_means_18, k_means_19, k_means_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "CH_score = []\n",
    "\n",
    "for model in k_list:\n",
    "    labels = model.labels_\n",
    "    CH_score.append(calinski_harabasz_score(grouped_users, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to decide if I keep going with more clusters\n",
    "# Previous k_means_20 on unscaled data had CH around 16000, now scaled it is at 8000\n",
    "\n",
    "plt.plot([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], CH_score)\n",
    "plt.xticks([10,11,12,13,14,15,16,17,18,19,20])\n",
    "plt.title('Calinski Harabasz Scores for Different Values of K')\n",
    "plt.ylabel('Variance Ratio')\n",
    "plt.xlabel('K=')\n",
    "plt.savefig('Images/ch_scaled_scores.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at Within Cluster Sum of Squares\n",
    "\n",
    "wcss_score = []\n",
    "\n",
    "for model in k_list:\n",
    "    labels = model.labels_\n",
    "    wcss_score.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], wcss_score)\n",
    "plt.xticks([10,11,12,13,14,15,16,17,18,19,20])\n",
    "plt.title('Within Cluster Sum of Squares Scores for Different Values of K')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xlabel('K=')\n",
    "plt.savefig('Images/wcss_scores.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette score 1 is good, -1 is bad, near 0 means overlapping custers\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.silhouette_score(grouped_users, k_means_20.labels_, sample_size = 30000, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.silhouette_score(grouped_users, k_means_19.labels_, sample_size = 30000, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's focus on k_means_20 and have a look at our clusters\n",
    "\n",
    "k_means_20.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster assignment to the grouped_users dataframe\n",
    "grouped_users['cluster'] = k_means_20.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# There are top 3 big clusters and lots of smaller.  May be hard to determine what the big clusters have in common.\n",
    "grouped_users.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now take the grouped users and group them by cluster\n",
    "\n",
    "# For each user, I took the mode of their order_dow.  Now I am taking the median value for the cluster.\n",
    "\n",
    "# cluster_data = grouped_users.groupby('cluster').agg({'num_orders': 'median', \n",
    "#                                                      'mode_order_dow': lambda x:x.value_counts().index[0], \n",
    "#                                                      'median_order_hour': 'median', 'mean_days_since': 'mean'})\n",
    "cluster_data = grouped_users.groupby('cluster').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data # This is the median info for each cluster\n",
    "\n",
    "# I can see cluster 7 has a lot of baby products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the users that make up cluster 7, and yes they have a lot of baby products\n",
    "\n",
    "grouped_users[grouped_users['cluster'] == 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These are all of cluster 7's values that aren't zero... still 62 of them\n",
    "\n",
    "cluster_data.iloc[7,(cluster_data.loc[7].values > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TSNE to convert cluster data to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to three dimensional for graphing\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "cluster_embedded = TSNE(n_components=3).fit_transform(cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to plot\n",
    "cluster_embedded_df = pd.DataFrame(cluster_embedded, index = cluster_data.index, columns = ['1','2','3'])\n",
    "cluster_embedded_df.reset_index(inplace=True)\n",
    "cluster_embedded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# axes instance\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "# get colormap from seaborn\n",
    "cmap = ListedColormap(sns.color_palette(\"husl\", 256).as_hex())\n",
    "\n",
    "# plot\n",
    "sc = ax.scatter(cluster_embedded_df['1'], cluster_embedded_df['2'], cluster_embedded_df['3'], \n",
    "                s=40, c=cluster_embedded_df['cluster'], marker='o', cmap=cmap, alpha=1)\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "# legend\n",
    "plt.legend(*sc.legend_elements(num=20), bbox_to_anchor=(1.05, 1), loc=2)\n",
    "\n",
    "# save\n",
    "plt.savefig(\"Images/scatter_hue.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows clusters 5, 11, 12, 14, and 17 as being very separate from the rest\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(cluster_embedded_df, x='1', y='2', z='3', color='cluster')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can I plot all of the users with their cluster color?\n",
    "# This took about 2 hours and 20 minutes to run.\n",
    "\n",
    "# user_embedded = TSNE(n_components=3).fit_transform(grouped_users.drop(columns='cluster'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(user_embedded, open(\"Pickle/user_embedded.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedded = pickle.load(open(\"Pickle/user_embedded.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to plot\n",
    "user_embedded_df = pd.DataFrame(user_embedded, index = grouped_users.index, columns = ['1','2','3'])\n",
    "user_embedded_df.reset_index(inplace=True)\n",
    "user_embedded_df['cluster'] = k_means_20.labels_\n",
    "user_embedded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(user_embedded_df.sample(200), x='1', y='2', z='3', color='cluster')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Breakdown by Aisle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a way to compare clusters.  Find if they have max or min values for any features.\n",
    "\n",
    "cluster_metrics = {}\n",
    "for cluster in cluster_data.index:\n",
    "    cluster_list = [grouped_users.cluster.value_counts()[cluster]]\n",
    "    for col in cluster_data.columns:\n",
    "        if (cluster_data.loc[:,col].max() > 0) & (cluster_data.loc[:,col].idxmax() == cluster):\n",
    "            cluster_list.append(('max ' + col, cluster_data.loc[cluster,col]))\n",
    "        if (cluster_data.loc[:,col].min() > 0) & (cluster_data.loc[:,col].idxmin() == cluster):\n",
    "                cluster_list.append(('min ' + col, cluster_data.loc[cluster,col]))\n",
    "    cluster_metrics[cluster] = cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluster_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clusters 5, 11, 12, 14, and 17 looked very separate from the rest on the graph.\n",
    "\n",
    "print(cluster_metrics[5]) # Lots of personal care / pharmacy type products\n",
    "print(cluster_metrics[11]) # Soap and skin care\n",
    "print(cluster_metrics[12]) # Very large cluster, with fewest number of orders and highest days between orders\n",
    "print(cluster_metrics[14]) # Tons of veggies, herb, and spices\n",
    "print(cluster_metrics[17]) # Bulk dried fruits and veggies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Other interesting clusters\n",
    "\n",
    "print(cluster_metrics[7]) # Baby products\n",
    "print(cluster_metrics[8]) # Lots of orders, shortest days between, big buyers\n",
    "print(cluster_metrics[9]) # Alcohol purchasers\n",
    "print(cluster_metrics[13]) # Household, laundry, cleaning products\n",
    "print(cluster_metrics[15]) # Chocolate, gum and soft drinks, least veggies\n",
    "print(cluster_metrics[16]) # Vegan and tofu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a heatmap of clusters and aisles\n",
    "# Scale the data first to make it more meaningful\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "graph_data = scaler.fit_transform(cluster_data)\n",
    "graph_df=pd.DataFrame(graph_data, columns = cluster_data.columns)\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "heat_map = sns.heatmap(graph_df, cmap=\"YlGnBu\")\n",
    "plt.savefig('Images/cluster_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can I write a function to predict what cluster someone will belong in?\n",
    "# They would have to give me a shopping list... no even then some clusters may be based on order frequency rather than item.\n",
    "# Or rather, if they give me an item, can I ouput \"Others who bought this item also bought...\"\n",
    "# That we will get from the recommendation system below I think"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Buying Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add up all of the products for each person\n",
    "\n",
    "grouped_users['num_products'] = grouped_users[grouped_users.columns[4:-1]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be used to count how many users are in each cluster when I do the groupby\n",
    "grouped_users['user_count'] = list(np.ones(len(grouped_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_users = movecol(grouped_users, \n",
    "                        cols_to_move=['num_products', 'user_count', 'cluster'], \n",
    "                        ref_col='mode_order_dow', \n",
    "                        place='Before')\n",
    "grouped_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster, adding up the number of products purchased\n",
    "grouped_clusters = grouped_users.groupby('cluster').sum()\n",
    "grouped_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't take into account the relative prices of the items purchased\n",
    "# But we can now see the portion of products purchased by each cluster\n",
    "cluster_power = grouped_clusters.iloc[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ordering statistics per cluster\n",
    "cluster_power['portion_of_orders'] = cluster_power['num_orders'].apply(lambda x: \n",
    "                                                                             x/(cluster_power['num_orders'].sum()))\n",
    "cluster_power['portion_of_products'] = cluster_power['num_products'].apply(lambda x: \n",
    "                                                                                 x/(cluster_power['num_products'].sum()))\n",
    "cluster_power['portion_of_users'] = cluster_power['user_count'].apply(lambda x: \n",
    "                                                                             x/(cluster_power['user_count'].sum()))\n",
    "cluster_power['orders_per_user'] = cluster_power['num_orders']/cluster_power['user_count']\n",
    "cluster_power['products_per_user'] = cluster_power['num_products']/cluster_power['user_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_power.sort_values('products_per_user', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sorting these statistics different ways shows interesting results depending on what you are looking for.  We can see\n",
    "that cluster 8 orders a very large number of products per user, but overall cluster 8 represents a small portion of all\n",
    "of the users.  Cluster 12 represents over 50% of all of the users, but only 25% of the orders and only 17% of the products.\n",
    "Cluster 1 is very proportional with about 23% of the users, orders, and products.  Cluster 4 is the third largest cluster\n",
    "with 7% of the users, but they make up about 17% of the orders and products.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Metadata search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products_desc = pickle.load(open(\"Pickle/products_desc.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the metadata and fit to a vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_desc['metadata'] = products_desc.apply(lambda x : x['aisle']+' '+x['department']+' '+x['product_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(stop_words='english')\n",
    "count_vec_matrix = count_vec.fit_transform(products_desc['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in any words and vectorizes them, then find similar vector in the count_vec_matrix\n",
    "\n",
    "def vectorize_products_based_on_metadata(product_input):\n",
    "\n",
    "    vec = count_vec.transform(pd.Series(product_input))\n",
    "    \n",
    "    simil = cosine_similarity(vec, count_vec_matrix)\n",
    "    \n",
    "    simil_scores = pd.DataFrame(simil.reshape(49688,), index = products_desc.index, columns=['score'])\n",
    "    \n",
    "    # Don't return scores of zero, only as many positive scores as exist\n",
    "    non_zero_scores = simil_scores[simil_scores['score'] > 0]\n",
    "    \n",
    "    if len(non_zero_scores) == 0:\n",
    "        print('No similar products found.  Please refine your search terms and try again')\n",
    "        return\n",
    "    \n",
    "    if len(non_zero_scores) < 10:\n",
    "        item_count = len(non_zero_scores)\n",
    "    else:\n",
    "        item_count = 10\n",
    "    \n",
    "    similarity_scores = simil_scores.sort_values(['score'], ascending=False)[:item_count]\n",
    "    \n",
    "    return (products_desc['product_name'].iloc[similarity_scores.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorize_products_based_on_metadata('Bubble Bath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorize_products_based_on_metadata('Oreo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorize_products_based_on_metadata('Oreos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_products_based_on_metadata('Oreos Cookies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectorize_products_based_on_metadata('Premium Almonds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'd rather put more weight on the noun and less on the adjective\n",
    "\n",
    "vectorize_products_based_on_metadata('Red Potatoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorize_products_based_on_metadata('randomword')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem the product metadata and refit\n",
    "\n",
    "These are mostly proper names of products so I don't think I want to lemmatize as that may change the product name too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stem_list=[]\n",
    "for i in range(len(products_desc['metadata'])):\n",
    "    word_list = nltk.word_tokenize(products_desc['metadata'][i])\n",
    "    stem_set = list(set([stemmer.stem(word) for word in word_list]))\n",
    "    stem_list.append(' '.join(stem_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_desc['stemmed'] = stem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the vectorizer\n",
    "\n",
    "stem_count_vec_matrix = count_vec.fit_transform(products_desc['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This improved function takes in any words and stems and vectorizes them\n",
    "\n",
    "def stem_and_vectorize_products_based_on_metadata(product_input):\n",
    "\n",
    "    word_list = nltk.word_tokenize(product_input)\n",
    "    input_stemmed = [stemmer.stem(word) for word in word_list]\n",
    "    vec = count_vec.transform(pd.Series(input_stemmed))\n",
    "    \n",
    "    simil = cosine_similarity(vec, stem_count_vec_matrix)\n",
    "    \n",
    "    simil_scores = pd.DataFrame(simil.reshape(stem_count_vec_matrix.shape[0],), \n",
    "                                index = products_desc.index, columns=['score'])\n",
    "    \n",
    "    # Don't return scores of zero, only as many positive scores as exist\n",
    "    non_zero_scores = simil_scores[simil_scores['score'] > 0]\n",
    "    \n",
    "    if len(non_zero_scores) == 0:\n",
    "        print('No similar products found.  Please refine your search terms and try again')\n",
    "        return\n",
    "    \n",
    "    if len(non_zero_scores) < 10:\n",
    "        item_count = len(non_zero_scores)\n",
    "    else:\n",
    "        item_count = 10\n",
    "    \n",
    "    similarity_scores = simil_scores.sort_values(['score'], ascending=False)[:item_count]\n",
    "    \n",
    "    return (products_desc['product_name'].iloc[similarity_scores.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stem_and_vectorize_products_based_on_metadata('Oreos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stem_and_vectorize_products_based_on_metadata('Oreo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "333.778px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
